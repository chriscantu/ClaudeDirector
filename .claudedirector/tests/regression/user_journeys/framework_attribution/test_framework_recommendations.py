#!/usr/bin/env python3
"""
ðŸ’¡ Framework Recommendations Regression Test - Critical User Journey 5d/5

BUSINESS CRITICAL PATH: Proactive framework recommendations and usage analytics
FAILURE IMPACT: Framework recommendations lost, usage insights missing, proactive guidance broken

This focused test suite validates framework recommendation engine and analytics:
1. Proactive framework recommendations based on user context
2. Framework usage analytics and pattern tracking
3. Recommendation confidence scoring and rationale
4. End-to-end framework attribution workflow validation

COVERAGE: Framework recommendation intelligence validation
PRIORITY: P0 HIGH - Proactive framework guidance and analytics
EXECUTION: <3 seconds for complete recommendation validation
"""

import sys
import os
import unittest
import tempfile
import time
from pathlib import Path
from datetime import datetime, timedelta

# Add the ClaudeDirector lib to path for testing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../../../lib"))


class TestFrameworkRecommendations(unittest.TestCase):
    """Test framework recommendation engine and usage analytics"""

    def setUp(self):
        """Set up test environment for recommendations testing"""
        self.test_dir = tempfile.mkdtemp()

        # Framework library for recommendations
        self.strategic_frameworks = {
            "Team Topologies": {
                "category": "organizational",
                "keywords": ["team", "cognitive", "load", "boundaries", "platform"],
                "personas": ["diego", "martin", "rachel"],
                "application_context": ["organizational_design", "team_structure"],
            },
            "Capital Allocation Framework": {
                "category": "investment_strategy",
                "keywords": ["investment", "allocation", "roi", "platform", "resource"],
                "personas": ["alvaro", "diego", "camille"],
                "application_context": ["platform_investment", "resource_allocation"],
            },
            "Design System Maturity Model": {
                "category": "design_systems",
                "keywords": [
                    "design",
                    "system",
                    "component",
                    "consistency",
                    "maturity",
                ],
                "personas": ["rachel", "martin", "diego"],
                "application_context": ["design_systems", "component_architecture"],
            },
            "Technical Strategy Framework": {
                "category": "technical_strategy",
                "keywords": ["technical", "architecture", "roadmap", "technology"],
                "personas": ["martin", "diego", "camille"],
                "application_context": ["technical_strategy", "architecture_decisions"],
            },
            "Crucial Conversations": {
                "category": "communication",
                "keywords": ["stakeholder", "conversation", "alignment", "conflict"],
                "personas": ["diego", "rachel", "camille"],
                "application_context": [
                    "stakeholder_management",
                    "executive_communication",
                ],
            },
        }

        # Recommendation test scenarios
        self.recommendation_scenarios = [
            {
                "name": "Platform Leadership Context",
                "user_context": {
                    "recent_topics": ["team_scaling", "platform_investment"],
                    "persona_preference": "diego",
                    "organization_context": "UI_Foundation",
                    "current_initiatives": ["platform_modernization"],
                },
                "expected_recommendations": [
                    "Team Topologies",
                    "Capital Allocation Framework",
                ],
                "recommendation_confidence": 0.7,
            },
            {
                "name": "Design System Leadership Context",
                "user_context": {
                    "recent_topics": [
                        "design_system",
                        "accessibility",
                        "user_experience",
                    ],
                    "persona_preference": "rachel",
                    "organization_context": "Design_Platform",
                    "current_initiatives": ["design_system_scaling"],
                },
                "expected_recommendations": ["Design System Maturity Model"],
                "recommendation_confidence": 0.8,
            },
            {
                "name": "Technical Architecture Context",
                "user_context": {
                    "recent_topics": [
                        "architecture",
                        "technical_debt",
                        "platform_roadmap",
                    ],
                    "persona_preference": "martin",
                    "organization_context": "Platform_Engineering",
                    "current_initiatives": ["architecture_modernization"],
                },
                "expected_recommendations": ["Technical Strategy Framework"],
                "recommendation_confidence": 0.75,
            },
            {
                "name": "Investment Strategy Context",
                "user_context": {
                    "recent_topics": [
                        "roi_analysis",
                        "investment_strategy",
                        "platform_value",
                    ],
                    "persona_preference": "alvaro",
                    "organization_context": "Platform_Investment",
                    "current_initiatives": ["budget_planning"],
                },
                "expected_recommendations": ["Capital Allocation Framework"],
                "recommendation_confidence": 0.75,
            },
        ]

    def tearDown(self):
        """Clean up test environment"""
        import shutil

        shutil.rmtree(self.test_dir, ignore_errors=True)

    def test_framework_recommendation_engine(self):
        """REGRESSION TEST: Framework recommendation engine suggests appropriate frameworks proactively"""
        try:
            for scenario in self.recommendation_scenarios:
                scenario_name = scenario["name"]
                user_context = scenario["user_context"]
                expected_recs = scenario["expected_recommendations"]
                min_confidence = scenario["recommendation_confidence"]

                # Generate proactive recommendations
                recommendations = self._generate_framework_recommendations(user_context)

                # Verify recommendations are provided
                self.assertGreater(
                    len(recommendations),
                    0,
                    f"{scenario_name}: Should provide framework recommendations for context: {user_context}",
                )

                # Verify expected frameworks are recommended
                recommended_names = [rec["framework"] for rec in recommendations]
                for expected_framework in expected_recs:
                    self.assertIn(
                        expected_framework,
                        recommended_names,
                        f"{scenario_name}: Should recommend '{expected_framework}' for context: {user_context['recent_topics']}",
                    )

                # Verify recommendation confidence
                for rec in recommendations:
                    if rec["framework"] in expected_recs:
                        self.assertGreaterEqual(
                            rec["confidence"],
                            min_confidence,
                            f"{scenario_name}: Recommendation confidence for '{rec['framework']}' should be >= {min_confidence}",
                        )

                # Verify recommendation rationale
                for rec in recommendations:
                    self.assertIn(
                        "rationale",
                        rec,
                        f"{scenario_name}: Recommendation for '{rec['framework']}' should include rationale",
                    )

                    self.assertGreater(
                        len(rec["rationale"]),
                        10,
                        f"{scenario_name}: Recommendation rationale should be meaningful: {rec['rationale']}",
                    )

        except Exception as e:
            self.fail(f"Framework recommendation engine test failed: {e}")

    def test_framework_usage_analytics(self):
        """REGRESSION TEST: Framework usage is tracked and analyzed for insights"""
        try:
            # Simulate framework usage over time
            usage_sessions = []

            # Generate usage data from recommendation scenarios
            for i, scenario in enumerate(self.recommendation_scenarios * 3):  # 3 rounds
                query = f"Strategic question {i} related to {scenario['user_context']['recent_topics'][0]}"

                # Simulate detected frameworks based on context
                detected_frameworks = self._simulate_framework_detection(
                    scenario["user_context"]
                )

                session_data = {
                    "session_id": f"session_{i}",
                    "timestamp": datetime.now() + timedelta(hours=i),
                    "persona": scenario["user_context"]["persona_preference"],
                    "context": scenario["name"],
                    "frameworks_detected": list(detected_frameworks.keys()),
                    "confidence_scores": {
                        name: data["confidence"]
                        for name, data in detected_frameworks.items()
                    },
                    "user_feedback": 0.8 + (i % 3) * 0.1,  # Simulated user satisfaction
                }

                usage_sessions.append(session_data)

            # Analyze usage patterns
            analytics_result = self._analyze_framework_usage(usage_sessions)

            # Verify analytics components
            self.assertIn(
                "most_used_frameworks",
                analytics_result,
                "Analytics should track most used frameworks",
            )

            self.assertIn(
                "persona_preferences",
                analytics_result,
                "Analytics should track persona framework preferences",
            )

            self.assertIn(
                "context_applications",
                analytics_result,
                "Analytics should track framework context applications",
            )

            self.assertIn(
                "confidence_trends",
                analytics_result,
                "Analytics should track confidence score trends",
            )

            # Verify analytics accuracy
            most_used = analytics_result["most_used_frameworks"]
            self.assertGreater(
                len(most_used), 0, "Should identify most used frameworks"
            )

            # Verify persona preferences tracking
            persona_prefs = analytics_result["persona_preferences"]
            for persona in ["diego", "rachel", "martin", "alvaro", "camille"]:
                if persona in persona_prefs:
                    self.assertGreater(
                        len(persona_prefs[persona]),
                        0,
                        f"Persona {persona} should have framework preferences tracked",
                    )

            # Verify confidence trends
            confidence_trends = analytics_result["confidence_trends"]
            self.assertIn(
                "average_confidence",
                confidence_trends,
                "Should track average confidence trends",
            )

            avg_confidence = confidence_trends["average_confidence"]
            self.assertGreaterEqual(
                avg_confidence,
                0.7,
                f"Average framework confidence should be >= 70%, got {avg_confidence:.2%}",
            )

        except Exception as e:
            self.fail(f"Framework usage analytics test failed: {e}")

    def test_recommendation_personalization(self):
        """REGRESSION TEST: Recommendations are personalized for different personas"""
        try:
            # Test persona-specific recommendations
            base_context = {
                "recent_topics": ["platform", "strategy", "team"],
                "organization_context": "Engineering",
                "current_initiatives": ["modernization"],
            }

            persona_tests = [
                {
                    "persona": "diego",
                    "expected_frameworks": [
                        "Team Topologies",
                        "Technical Strategy Framework",
                    ],
                    "context_focus": "leadership_and_organization",
                },
                {
                    "persona": "rachel",
                    "expected_frameworks": ["Design System Maturity Model"],
                    "context_focus": "design_and_user_experience",
                },
                {
                    "persona": "martin",
                    "expected_frameworks": [
                        "Technical Strategy Framework",
                        "Team Topologies",
                    ],
                    "context_focus": "technical_architecture",
                },
                {
                    "persona": "alvaro",
                    "expected_frameworks": ["Capital Allocation Framework"],
                    "context_focus": "investment_and_business",
                },
            ]

            for test in persona_tests:
                persona = test["persona"]
                expected_frameworks = test["expected_frameworks"]

                # Create persona-specific context
                persona_context = base_context.copy()
                persona_context["persona_preference"] = persona

                # Generate recommendations
                recommendations = self._generate_framework_recommendations(
                    persona_context
                )
                recommended_names = [rec["framework"] for rec in recommendations]

                # Verify persona-appropriate frameworks are recommended
                persona_appropriate_count = sum(
                    1
                    for framework in expected_frameworks
                    if framework in recommended_names
                )

                self.assertGreater(
                    persona_appropriate_count,
                    0,
                    f"Persona {persona} should receive appropriate framework recommendations. Got: {recommended_names}",
                )

                # Verify persona alignment
                for rec in recommendations:
                    framework_name = rec["framework"]
                    if framework_name in self.strategic_frameworks:
                        framework_personas = self.strategic_frameworks[framework_name][
                            "personas"
                        ]
                        if rec["confidence"] > 0.7:  # High confidence recommendations
                            self.assertIn(
                                persona,
                                framework_personas,
                                f"High-confidence framework '{framework_name}' should align with persona '{persona}'",
                            )

        except Exception as e:
            self.fail(f"Recommendation personalization test failed: {e}")

    def test_end_to_end_framework_workflow(self):
        """REGRESSION TEST: Complete end-to-end framework attribution workflow"""
        try:
            # Phase 1: Strategic Question Input
            strategic_question = """
            How should we approach the organizational transformation to scale our platform capabilities
            while managing technical debt, ensuring design system consistency, and maintaining stakeholder
            confidence through this complex multi-year initiative?
            """

            # Phase 2: Framework Detection (simulated)
            detected_frameworks = {
                "Team Topologies": {"confidence": 0.85, "keyword_matches": 4},
                "Technical Strategy Framework": {
                    "confidence": 0.80,
                    "keyword_matches": 3,
                },
                "Design System Maturity Model": {
                    "confidence": 0.75,
                    "keyword_matches": 3,
                },
            }

            self.assertGreater(
                len(detected_frameworks),
                0,
                "Should detect frameworks for complex strategic question",
            )

            # Phase 3: Framework Coordination
            coordination_result = self._coordinate_frameworks(detected_frameworks)
            self.assertIn(
                "primary_framework",
                coordination_result,
                "Should identify primary framework",
            )

            primary_framework = coordination_result["primary_framework"]
            supporting_frameworks = coordination_result["supporting_frameworks"]

            # Phase 4: Attribution Generation
            attribution_text = self._generate_attribution(detected_frameworks)
            self.assertIn(
                "ðŸ“š Strategic Framework:",
                attribution_text,
                "Should generate properly formatted attribution",
            )

            # Phase 5: Usage Tracking
            usage_data = {
                "timestamp": datetime.now().isoformat(),
                "question": strategic_question,
                "frameworks_detected": list(detected_frameworks.keys()),
                "primary_framework": primary_framework,
                "supporting_frameworks": supporting_frameworks,
                "confidence_scores": {
                    name: data["confidence"]
                    for name, data in detected_frameworks.items()
                },
                "attribution_generated": attribution_text,
            }

            # Phase 6: Future Recommendations
            user_context = {
                "recent_frameworks": list(detected_frameworks.keys()),
                "primary_topics": ["organizational_transformation", "platform_scaling"],
                "success_indicators": [
                    "high_confidence_detection",
                    "multi_framework_coordination",
                ],
            }

            future_recommendations = self._generate_framework_recommendations(
                user_context
            )
            self.assertGreater(
                len(future_recommendations),
                0,
                "Should generate recommendations for future strategic questions",
            )

            # Phase 7: End-to-End Validation
            workflow_success_criteria = [
                len(detected_frameworks) >= 2,  # Multi-framework detection
                primary_framework
                in self.strategic_frameworks,  # Valid primary framework
                "ðŸ“š Strategic Framework:" in attribution_text,  # Proper attribution
                len(future_recommendations) > 0,  # Proactive recommendations
                all(
                    score >= 0.6 for score in usage_data["confidence_scores"].values()
                ),  # Reasonable confidence
            ]

            workflow_success_rate = sum(workflow_success_criteria) / len(
                workflow_success_criteria
            )
            self.assertGreaterEqual(
                workflow_success_rate,
                0.8,
                f"End-to-end workflow success rate should be >= 80%, got {workflow_success_rate:.2%}",
            )

        except Exception as e:
            self.fail(f"End-to-end framework workflow test failed: {e}")

    # Helper methods for framework recommendations and analytics

    def _generate_framework_recommendations(self, user_context):
        """Generate proactive framework recommendations"""
        recommendations = []
        recent_topics = user_context.get("recent_topics", [])
        persona = user_context.get("persona_preference", "diego")

        # Score frameworks based on context
        for framework_name, framework_info in self.strategic_frameworks.items():
            if persona in framework_info["personas"]:
                # Calculate relevance score
                topic_matches = sum(
                    1
                    for topic in recent_topics
                    if any(keyword in topic for keyword in framework_info["keywords"])
                )
                relevance_score = topic_matches / max(len(recent_topics), 1)

                if relevance_score > 0.3:  # Threshold for recommendation
                    recommendations.append(
                        {
                            "framework": framework_name,
                            "confidence": min(0.5 + relevance_score, 1.0),
                            "rationale": f"Recommended based on recent topics: {', '.join(recent_topics[:2])} and persona expertise",
                        }
                    )

        return sorted(recommendations, key=lambda x: x["confidence"], reverse=True)

    def _simulate_framework_detection(self, user_context):
        """Simulate framework detection based on user context"""
        recent_topics = user_context.get("recent_topics", [])
        detected = {}

        for framework_name, framework_info in self.strategic_frameworks.items():
            keywords = framework_info["keywords"]

            # Check topic relevance
            topic_matches = sum(
                1
                for topic in recent_topics
                if any(keyword in topic for keyword in keywords)
            )

            if topic_matches > 0:
                confidence = min(0.6 + (topic_matches * 0.1), 0.9)
                detected[framework_name] = {
                    "confidence": confidence,
                    "keyword_matches": topic_matches,
                }

        return detected

    def _analyze_framework_usage(self, usage_sessions):
        """Simulate framework usage analytics"""
        framework_counts = {}
        persona_frameworks = {}
        context_frameworks = {}
        confidence_scores = []

        for session in usage_sessions:
            # Count framework usage
            for framework in session["frameworks_detected"]:
                framework_counts[framework] = framework_counts.get(framework, 0) + 1

            # Track persona preferences
            persona = session["persona"]
            if persona not in persona_frameworks:
                persona_frameworks[persona] = {}
            for framework in session["frameworks_detected"]:
                persona_frameworks[persona][framework] = (
                    persona_frameworks[persona].get(framework, 0) + 1
                )

            # Track context applications
            context = session["context"]
            if context not in context_frameworks:
                context_frameworks[context] = []
            context_frameworks[context].extend(session["frameworks_detected"])

            # Collect confidence scores
            confidence_scores.extend(session["confidence_scores"].values())

        return {
            "most_used_frameworks": sorted(
                framework_counts.items(), key=lambda x: x[1], reverse=True
            ),
            "persona_preferences": persona_frameworks,
            "context_applications": context_frameworks,
            "confidence_trends": {
                "average_confidence": (
                    sum(confidence_scores) / len(confidence_scores)
                    if confidence_scores
                    else 0
                ),
                "confidence_distribution": confidence_scores,
            },
        }

    def _coordinate_frameworks(self, detected_frameworks):
        """Simulate framework coordination logic"""
        frameworks_by_confidence = sorted(
            detected_frameworks.items(), key=lambda x: x[1]["confidence"], reverse=True
        )

        primary_framework = frameworks_by_confidence[0][0]
        supporting_frameworks = [name for name, _ in frameworks_by_confidence[1:3]]

        return {
            "primary_framework": primary_framework,
            "supporting_frameworks": supporting_frameworks,
            "coordination_rationale": f"Primary framework '{primary_framework}' selected based on highest confidence",
        }

    def _generate_attribution(self, detected_frameworks):
        """Generate framework attribution text"""
        if len(detected_frameworks) == 1:
            framework_name = list(detected_frameworks.keys())[0]
            return f"ðŸ“š Strategic Framework: {framework_name} detected"
        else:
            primary = list(detected_frameworks.keys())[0]
            others = list(detected_frameworks.keys())[1:]
            return f"ðŸ“š Strategic Framework: {primary} detected (with {', '.join(others)} for additional context)"


if __name__ == "__main__":
    print("ðŸ’¡ Framework Recommendations Regression Test")
    print("=" * 50)
    print("Testing framework recommendation engine and usage analytics...")
    print()

    # Run the focused test suite
    unittest.main(verbosity=2, exit=False)

    print()
    print("âœ… FRAMEWORK RECOMMENDATIONS REGRESSION TESTS COMPLETE")
    print(
        "Framework recommendation intelligence and analytics protected against regressions"
    )
