"""
Performance benchmarks for task detection functionality.

This module provides comprehensive benchmarking for task AI detection,
measuring response times, memory usage, and throughput across different
content sizes and complexity levels.
"""

import time
from unittest.mock import Mock, patch
from typing import Callable, Dict, Any

try:
    from lib.ai_intelligence.context.intelligence_unified import TaskIntelligence
    from lib.core.config import ClaudeDirectorConfig

    IMPORTS_AVAILABLE = True
except ImportError:
    IMPORTS_AVAILABLE = False

try:
    import pytest

    PYTEST_AVAILABLE = True
except ImportError:
    PYTEST_AVAILABLE = False


class PerformanceBenchmarkTask:
    """Base class for task detection performance benchmarks."""

    def __init__(self):
        """Initialize benchmark environment."""
        self.results: Dict[str, Any] = {}

    def run_multiple_iterations(
        self, func: Callable, iterations: int = 10
    ) -> Dict[str, Any]:
        """Run function multiple times and collect timing statistics."""
        times = []

        for _ in range(iterations):
            start_time = time.perf_counter()
            try:
                func()
                end_time = time.perf_counter()
                times.append(end_time - start_time)
            except Exception as e:
                print(f"Benchmark iteration failed: {e}")
                times.append(float("inf"))  # Mark failed iterations

        # Filter out failed iterations for statistics
        valid_times = [t for t in times if t != float("inf")]

        if not valid_times:
            return {
                "avg_time": float("inf"),
                "min_time": float("inf"),
                "max_time": float("inf"),
                "success_rate": 0.0,
                "total_iterations": iterations,
            }

        return {
            "avg_time": sum(valid_times) / len(valid_times),
            "min_time": min(valid_times),
            "max_time": max(valid_times),
            "success_rate": len(valid_times) / iterations,
            "total_iterations": iterations,
        }


@pytest.mark.skipif(not IMPORTS_AVAILABLE, reason="Required modules not available")
class TestTaskDetectionPerformance:
    """Benchmark task detection performance across different scenarios."""

    def setup_method(self):
        """Set up benchmark environment."""
        self.benchmark = PerformanceBenchmarkTask()
        self.config = Mock(spec=ClaudeDirectorConfig)
        self.config.database_path = ":memory:"
        self.config.task_auto_create_threshold = 0.75
        self.config.task_priority_threshold = 0.8
        self.config.enable_caching = False

        # Test content of varying sizes and complexity
        self.simple_task = "Fix the login bug by Friday"

        self.complex_task = """
        Need to implement comprehensive user authentication system with:
        - Multi-factor authentication support
        - Social login integration (Google, Facebook, GitHub)
        - Password strength validation
        - Account lockout after failed attempts
        - Session management with refresh tokens
        - GDPR compliance for user data
        - Audit logging for security events
        This is urgent and needs to be completed by end of quarter.
        """

        self.enterprise_task = """
        Strategic initiative: Platform modernization and microservices migration

        CRITICAL DEADLINE: Q2 2024 board presentation

        Phase 1 Requirements (High Priority):
        - Assess current monolithic architecture dependencies
        - Design microservices domain boundaries using DDD principles
        - Implement API gateway with rate limiting and authentication
        - Set up service mesh for inter-service communication
        - Establish monitoring and observability stack (Prometheus, Grafana, Jaeger)

        Phase 2 Requirements (Medium Priority):
        - Migrate user service to new architecture
        - Implement event-driven architecture with message queues
        - Set up CI/CD pipeline for containerized deployments
        - Database sharding strategy for user data
        - Performance testing framework for load validation

        Phase 3 Requirements (Low Priority but nice to have):
        - Advanced analytics and business intelligence dashboard
        - Machine learning recommendation engine
        - International expansion support (i18n, multi-region)
        - Advanced security hardening and penetration testing

        Stakeholders: CTO, VP Engineering, Platform team leads, Security team
        Budget approved: $2M over 18 months
        Success metrics: 99.9% uptime, 50% faster deployment cycles, 30% cost reduction
        """

    @patch("claudedirector.intelligence.task.StrategicTaskManager")
    @patch("claudedirector.intelligence.task.IntelligentTaskDetector")
    def test_task_detection_response_time(self, mock_detector, mock_manager):
        """Benchmark task detection response times across content complexity."""
        # Setup mocks
        mock_detector_instance = Mock()
        mock_detector.return_value = mock_detector_instance
        mock_detector_instance.detect_tasks.return_value = [
            {"task": "Sample task", "confidence": 0.9, "priority": "high"}
        ]

        task_intel = TaskIntelligence(config=self.config)

        # Benchmark simple task using actual API method
        def benchmark_simple():
            return task_intel.get_my_tasks()

        simple_stats = self.benchmark.run_multiple_iterations(
            benchmark_simple, iterations=100
        )

        # Benchmark complex task
        def benchmark_complex():
            return task_intel.get_my_tasks()

        complex_stats = self.benchmark.run_multiple_iterations(
            benchmark_complex, iterations=50
        )

        # Benchmark enterprise-level task
        def benchmark_enterprise():
            return task_intel.get_my_tasks()

        enterprise_stats = self.benchmark.run_multiple_iterations(
            benchmark_enterprise, iterations=20
        )

        # Performance assertions
        assert (
            simple_stats["avg_time"] < 0.05
        ), f"Simple task detection too slow: {simple_stats['avg_time']}s"
        assert (
            complex_stats["avg_time"] < 0.15
        ), f"Complex task detection too slow: {complex_stats['avg_time']}s"
        assert (
            enterprise_stats["avg_time"] < 0.5
        ), f"Enterprise task detection too slow: {enterprise_stats['avg_time']}s"

        # Success rate assertions
        assert (
            simple_stats["success_rate"] >= 0.95
        ), f"Simple task success rate too low: {simple_stats['success_rate']}"
        assert (
            complex_stats["success_rate"] >= 0.90
        ), f"Complex task success rate too low: {complex_stats['success_rate']}"
        assert (
            enterprise_stats["success_rate"] >= 0.85
        ), f"Enterprise task success rate too low: {enterprise_stats['success_rate']}"

        # Store results for reporting
        self.benchmark.results["task_detection"] = {
            "simple_task": simple_stats,
            "complex_task": complex_stats,
            "enterprise_task": enterprise_stats,
        }

        # Performance reporting
        print(f"\nðŸ“Š Task Detection Performance:")
        print(f"Simple task (avg): {simple_stats['avg_time']:.4f}s")
        print(f"Complex task (avg): {complex_stats['avg_time']:.4f}s")
        print(f"Enterprise task (avg): {enterprise_stats['avg_time']:.4f}s")

    @patch("claudedirector.intelligence.task.StrategicTaskManager")
    @patch("claudedirector.intelligence.task.IntelligentTaskDetector")
    def test_task_priority_detection_performance(self, mock_detector, mock_manager):
        """Benchmark task priority detection accuracy and speed."""
        # Setup mocks for priority detection
        mock_detector_instance = Mock()
        mock_detector.return_value = mock_detector_instance

        # Mock different priority levels
        mock_detector_instance.detect_tasks.return_value = [
            {
                "task": "Critical security fix",
                "confidence": 0.95,
                "priority": "critical",
            },
            {"task": "Nice to have feature", "confidence": 0.7, "priority": "low"},
            {"task": "Important refactoring", "confidence": 0.85, "priority": "high"},
        ]

        task_intel = TaskIntelligence(config=self.config)

        # Test priority detection patterns
        priority_patterns = [
            ("URGENT: Fix production outage", "critical"),
            ("Critical security vulnerability", "critical"),
            ("High priority: Database optimization", "high"),
            ("Important: Update documentation", "high"),
            ("Nice to have: Dark mode toggle", "low"),
            ("Optional: Add user avatars", "low"),
            ("Medium priority: Refactor utilities", "medium"),
            ("Regular maintenance task", "medium"),
        ]

        def benchmark_priority_detection():
            # Simulate priority detection logic
            results = []
            for pattern, expected in priority_patterns:
                detected = task_intel.get_my_tasks()
                results.append((pattern, expected, detected))
            return results

        priority_stats = self.benchmark.run_multiple_iterations(
            benchmark_priority_detection, iterations=30
        )

        # Performance assertions for priority detection
        assert (
            priority_stats["avg_time"] < 0.2
        ), f"Priority detection too slow: {priority_stats['avg_time']}s"
        assert (
            priority_stats["success_rate"] >= 0.90
        ), f"Priority detection success rate too low: {priority_stats['success_rate']}"

        # Store results
        self.benchmark.results["priority_detection"] = priority_stats

        print(f"\nðŸŽ¯ Priority Detection Performance:")
        print(f"Priority detection (avg): {priority_stats['avg_time']:.4f}s")
        print(f"Success rate: {priority_stats['success_rate']:.2%}")

    @patch("claudedirector.intelligence.task.StrategicTaskManager")
    def test_task_storage_performance(self, mock_manager):
        """Benchmark task storage and retrieval operations."""
        # Setup mock for storage operations
        mock_manager_instance = Mock()
        mock_manager.return_value = mock_manager_instance
        mock_manager_instance.get_my_tasks.return_value = []
        mock_manager_instance.get_overdue_tasks.return_value = []

        task_intel = TaskIntelligence(config=self.config)

        # Benchmark task retrieval
        def benchmark_task_retrieval():
            my_tasks = task_intel.get_my_tasks()
            overdue_tasks = task_intel.get_overdue_tasks()
            return len(my_tasks) + len(overdue_tasks)

        retrieval_stats = self.benchmark.run_multiple_iterations(
            benchmark_task_retrieval, iterations=200
        )

        # Performance assertions for storage operations
        assert (
            retrieval_stats["avg_time"] < 0.01
        ), f"Task retrieval too slow: {retrieval_stats['avg_time']}s"
        assert (
            retrieval_stats["success_rate"] >= 0.98
        ), f"Task retrieval success rate too low: {retrieval_stats['success_rate']}"

        # Store results
        self.benchmark.results["storage_operations"] = retrieval_stats

        print(f"\nðŸ’¾ Storage Performance:")
        print(f"Task retrieval (avg): {retrieval_stats['avg_time']:.4f}s")
        print(f"Success rate: {retrieval_stats['success_rate']:.2%}")

    def test_memory_usage_monitoring(self):
        """Monitor memory usage during task detection operations."""
        import psutil
        import os

        process = psutil.Process(os.getpid())

        # Baseline memory usage
        baseline_memory = process.memory_info().rss / 1024 / 1024  # MB

        # Create task intelligence instance
        config = Mock(spec=ClaudeDirectorConfig)
        config.database_path = ":memory:"
        task_intel = TaskIntelligence(config=config)

        # Perform memory-intensive operations
        for _ in range(100):
            task_intel.get_my_tasks()

        # Check memory usage after operations
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - baseline_memory

        # Memory usage assertions
        assert (
            memory_increase < 50
        ), f"Memory usage increased too much: {memory_increase:.2f}MB"

        print(f"\nðŸ§  Memory Usage:")
        print(f"Baseline: {baseline_memory:.2f}MB")
        print(f"Final: {final_memory:.2f}MB")
        print(f"Increase: {memory_increase:.2f}MB")


if __name__ == "__main__":
    # Run performance benchmarks directly
    if IMPORTS_AVAILABLE and PYTEST_AVAILABLE:
        pytest.main([__file__, "-v", "-s"])
    else:
        print("Required modules not available for performance testing")
