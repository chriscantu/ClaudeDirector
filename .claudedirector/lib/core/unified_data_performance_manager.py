#!/usr/bin/env python3
"""
ðŸŽ¯ PHASE 8.4: UNIFIED DATA & PERFORMANCE MANAGER - Maximum DRY Consolidation

ELIMINATES MASSIVE DUPLICATION by consolidating:
- unified_performance_manager.py (911 lines) â†’ ELIMINATED
- conversational_data_manager.py (943 lines) â†’ ELIMINATED
TOTAL ELIMINATION: 1,854+ lines of duplicate patterns

CONSOLIDATION STRATEGY:
- Single BaseManager inheritance eliminates ALL duplicate infrastructure
- Unified configuration, logging, caching, metrics patterns
- Combined performance optimization + conversational data functionality
- Maintains full API compatibility through delegation methods

Built on BaseManager pattern for maximum DRY compliance and NET code reduction.
"""

import asyncio
import time
import json
import re
from typing import Dict, Any, List, Optional, Union, Callable, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from concurrent.futures import ThreadPoolExecutor

# PHASE 8.4: Single BaseManager import eliminates duplicate infrastructure
from .base_manager import BaseManager, BaseManagerConfig, ManagerType


class PerformanceTarget(Enum):
    """Unified performance targets for all optimization levels"""

    ULTRA_FAST = 25  # Executive priority, ML inference
    FAST = 50  # Strategic queries, sub-50ms target
    NORMAL = 500  # Standard queries, <500ms guarantee
    BACKGROUND = 1000  # Background tasks, <1s acceptable

    # Legacy compatibility aliases for existing tests
    CRITICAL = 25  # Maps to ULTRA_FAST
    HIGH = 50  # Maps to FAST
    LOW = 1000  # Maps to BACKGROUND


class QueryType(Enum):
    """Types of conversational queries supported"""

    SPRINT_METRICS = "sprint_metrics"
    TEAM_PERFORMANCE = "team_performance"
    ROI_ANALYSIS = "roi_analysis"
    ARCHITECTURE_HEALTH = "architecture_health"
    DESIGN_SYSTEM_STATUS = "design_system_status"
    GITHUB_ACTIVITY = "github_activity"
    GENERAL_ANALYTICS = "general_analytics"


class ResponseStatus(Enum):
    """Unified response status for all response types"""

    SUCCESS = "success"
    PARTIAL_SUCCESS = "partial_success"
    FALLBACK = "fallback"
    ERROR = "error"
    TIMEOUT = "timeout"
    CACHED = "cached"


class ResponseType(Enum):
    """Response type classification for proper handling"""

    MCP_INTEGRATION = "mcp_integration"
    PERSONA_ENHANCED = "persona_enhanced"
    CONVERSATIONAL = "conversational"
    ML_PREDICTION = "ml_prediction"
    DATA_QUERY = "data_query"
    SYSTEMATIC_ANALYSIS = "systematic_analysis"
    LIGHTWEIGHT_FALLBACK = "lightweight_fallback"
    TRANSPARENCY = "transparency"
    CHAT_INTERFACE = "chat_interface"
    GENERIC = "generic"


@dataclass
class UnifiedResponse:
    """
    Unified response structure that replaces ALL scattered response classes
    Consolidates functionality from unified_response_handler.py (590 lines â†’ ELIMINATED)
    """

    # Core response data (required)
    content: str
    status: ResponseStatus
    response_type: ResponseType

    # Performance metrics (integrated)
    processing_time: float = 0.0
    performance_metrics: Optional[Dict[str, Any]] = None

    # Success/Error information
    success: bool = True
    error: Optional[str] = None
    error_code: Optional[str] = None

    # Additional metadata
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    request_id: Optional[str] = None


@dataclass
class ConversationalQuery:
    """Represents a parsed conversational query"""

    query_type: QueryType
    entities: List[str] = field(default_factory=list)
    time_range: Optional[str] = None
    filters: Dict[str, Any] = field(default_factory=dict)
    context: Dict[str, Any] = field(default_factory=dict)


@dataclass
class UnifiedDataPerformanceConfig(BaseManagerConfig):
    """Consolidated configuration for performance + conversational data"""

    # Performance optimization settings
    ultra_fast_target_ms: int = 25
    fast_target_ms: int = 50
    normal_target_ms: int = 500
    background_target_ms: int = 1000

    # Thread pool settings
    ultra_fast_workers: int = 2
    fast_workers: int = 4
    normal_workers: int = 8
    background_workers: int = 4

    # Conversational data settings
    query_cache_ttl: int = 300  # 5 minutes
    data_source_timeout: int = 10  # 10 seconds
    max_entities_per_query: int = 10


class UnifiedDataPerformanceManager(BaseManager):
    """
    ðŸŽ¯ PHASE 8.4: UNIFIED DATA & PERFORMANCE MANAGER

    MASSIVE CONSOLIDATION - Eliminates 1,854+ lines by combining:
    - Performance optimization (ultra-fast, fast, normal, background targets)
    - Conversational data parsing and fetching
    - Real-time analytics and query processing
    - Chat context management and persistence

    ALL duplicate infrastructure eliminated through BaseManager inheritance:
    - Single logging system (BaseManager.logger)
    - Single caching system (BaseManager.cache)
    - Single metrics system (BaseManager.metrics)
    - Single configuration pattern (BaseManagerConfig)
    """

    def __init__(self, config: Optional[UnifiedDataPerformanceConfig] = None):
        """Initialize unified manager with consolidated patterns"""
        if config is None:
            config = UnifiedDataPerformanceConfig(
                manager_name="unified_data_performance_manager",
                manager_type=ManagerType.PERFORMANCE,
                enable_caching=True,
                enable_performance_tracking=True,
                cache_ttl_seconds=300,
            )

        # Initialize BaseManager (eliminates ALL duplicate infrastructure)
        super().__init__(config)

        self.config: UnifiedDataPerformanceConfig = config

        # Initialize consolidated components
        self._initialize_performance_infrastructure()
        self._initialize_conversational_patterns()

    def manage(self, operation: str, *args, **kwargs) -> Any:
        """
        Unified BaseManager interface - handles both performance and conversational operations
        """
        # Performance operations
        if operation == "optimize_request":
            return asyncio.run(self.optimize_request(*args, **kwargs))
        elif operation == "process":
            return self.process(*args, **kwargs)

        # Conversational data operations
        elif operation == "parse_query":
            return asyncio.run(self.parse_conversational_query(*args, **kwargs))
        elif operation == "fetch_data":
            return asyncio.run(self.fetch_real_time_data(*args, **kwargs))
        elif operation == "process_query":
            return asyncio.run(self.process_conversational_query(*args, **kwargs))

        else:
            # Dynamic method delegation
            if hasattr(self, operation):
                method = getattr(self, operation)
                if asyncio.iscoroutinefunction(method):
                    return asyncio.run(method(*args, **kwargs))
                else:
                    return method(*args, **kwargs)
            else:
                raise ValueError(f"Unknown operation: {operation}")

    def _initialize_performance_infrastructure(self):
        """Initialize performance optimization components"""
        # Thread pools for different performance targets
        self.ultra_fast_executor = ThreadPoolExecutor(
            max_workers=self.config.ultra_fast_workers, thread_name_prefix="ultra_fast"
        )
        self.fast_executor = ThreadPoolExecutor(
            max_workers=self.config.fast_workers, thread_name_prefix="fast"
        )
        self.normal_executor = ThreadPoolExecutor(
            max_workers=self.config.normal_workers, thread_name_prefix="normal"
        )
        self.background_executor = ThreadPoolExecutor(
            max_workers=self.config.background_workers, thread_name_prefix="background"
        )

    def _initialize_conversational_patterns(self):
        """Initialize conversational query patterns"""
        self.query_patterns = {
            # Sprint & Team Metrics
            r"(sprint|iteration).*(?:metrics|performance|velocity)": {
                "type": QueryType.SPRINT_METRICS,
                "entities_pattern": r"(team|sprint|iteration)\s+(\w+)",
                "time_pattern": r"(last|past|current)\s+(\d+\s+\w+)",
            },
            r"team.*(?:performance|productivity|metrics)": {
                "type": QueryType.TEAM_PERFORMANCE,
                "entities_pattern": r"team\s+(\w+)",
                "time_pattern": r"(last|past|current)\s+(\d+\s+\w+)",
            },
            # ROI & Business Analytics
            r"(?:roi|return|investment|cost|budget).*analysis": {
                "type": QueryType.ROI_ANALYSIS,
                "entities_pattern": r"(project|initiative|feature)\s+(\w+)",
                "time_pattern": r"(quarter|q\d|year|\d{4})",
            },
            # Architecture & Technical Health
            r"(?:architecture|technical|system).*(?:health|status|metrics)": {
                "type": QueryType.ARCHITECTURE_HEALTH,
                "entities_pattern": r"(service|system|component)\s+(\w+)",
            },
            # Default fallback
            r".*": {
                "type": QueryType.GENERAL_ANALYTICS,
            },
        }

    async def optimize_request(
        self,
        request_func: Callable,
        target: PerformanceTarget = PerformanceTarget.NORMAL,
        context: Optional[Dict[str, Any]] = None,
        enable_cache: bool = True,
        request_id: Optional[str] = None,
    ) -> Tuple[Any, Dict[str, Any]]:
        """
        Unified performance optimization for any request type
        Consolidates all previous performance optimization patterns
        """
        start_time = time.time()

        # Cache key generation
        if enable_cache and request_id:
            cache_key = f"perf_{target.name}_{request_id}"
            cached_result = self.cache.get(cache_key)
            if cached_result:
                processing_time = time.time() - start_time
                self.metrics.update(
                    {
                        "processing_time_ms": processing_time * 1000,
                        "cache_hit": True,
                        "target_achieved": processing_time < (target.value / 1000),
                    }
                )
                return cached_result, self.metrics

        # Select appropriate executor based on target
        executor = {
            PerformanceTarget.ULTRA_FAST: self.ultra_fast_executor,
            PerformanceTarget.FAST: self.fast_executor,
            PerformanceTarget.NORMAL: self.normal_executor,
            PerformanceTarget.BACKGROUND: self.background_executor,
        }.get(target, self.normal_executor)

        try:
            # Execute with timeout based on target
            timeout = target.value / 1000  # Convert ms to seconds
            loop = asyncio.get_event_loop()

            result = await asyncio.wait_for(
                loop.run_in_executor(executor, request_func, context), timeout=timeout
            )

            # Cache successful results
            if enable_cache and request_id:
                self.cache[cache_key] = result

            # Update metrics
            processing_time = time.time() - start_time
            self.metrics.update(
                {
                    "processing_time_ms": processing_time * 1000,
                    "cache_hit": False,
                    "target_achieved": processing_time < (target.value / 1000),
                    "success": True,
                }
            )

            return result, self.metrics

        except asyncio.TimeoutError:
            self.logger.warning(f"Request timeout after {target.value}ms")
            processing_time = time.time() - start_time
            self.metrics.update(
                {
                    "processing_time_ms": processing_time * 1000,
                    "cache_hit": False,
                    "target_achieved": False,
                    "timeout": True,
                }
            )
            # Return graceful fallback
            return {"error": "Request timeout", "fallback": True}, self.metrics

    async def optimize_call(
        self,
        func: Callable,
        *args,
        priority: PerformanceTarget = PerformanceTarget.NORMAL,
        **kwargs,
    ) -> Any:
        """
        Legacy compatibility method for optimize_call interface
        Maps to optimize_request for backward compatibility with P0 tests
        """

        # Create a wrapper function that calls the original function with args
        def wrapper_func(context):
            if asyncio.iscoroutinefunction(func):
                return asyncio.run(func(*args, **kwargs))
            else:
                return func(*args, **kwargs)

        # Use optimize_request with legacy compatibility
        result, metrics = await self.optimize_request(
            wrapper_func,
            target=priority,
            context={"args": args, "kwargs": kwargs},
            enable_cache=True,
            request_id=f"legacy_call_{hash(str(args))}",
        )

        return result

    async def create_response(
        self,
        content: str,
        response_type: ResponseType,
        status: ResponseStatus = ResponseStatus.SUCCESS,
        **kwargs,
    ) -> UnifiedResponse:
        """
        Create a unified response with performance optimization
        Replaces unified_response_handler.py functionality (590 lines â†’ ELIMINATED)
        """
        start_time = time.time()
        request_id = kwargs.get("request_id", f"resp_{int(time.time() * 1000)}")

        try:
            # Performance optimization through unified manager
            if kwargs.get("enable_optimization", True):
                performance_target = self._get_response_performance_target(
                    response_type
                )

                # Create optimized response data
                response_data = {
                    "content": content,
                    "success": status
                    in [
                        ResponseStatus.SUCCESS,
                        ResponseStatus.PARTIAL_SUCCESS,
                        ResponseStatus.CACHED,
                    ],
                    **{
                        k: v
                        for k, v in kwargs.items()
                        if k not in ["enable_optimization", "request_id"]
                    },
                }
            else:
                response_data = {"content": content, "success": True}

            # Create unified response
            response = UnifiedResponse(
                content=response_data.get("content", content),
                status=status,
                response_type=response_type,
                processing_time=time.time() - start_time,
                performance_metrics=self.metrics,
                request_id=request_id,
                **{
                    k: v
                    for k, v in response_data.items()
                    if k not in ["content", "success"]
                },
            )

            # Update metrics
            self.metrics.update(
                {
                    "responses_created": self.metrics.get("responses_created", 0) + 1,
                    "response_creation_time_ms": response.processing_time * 1000,
                }
            )

            return response

        except Exception as e:
            # Unified error handling
            self.logger.error(f"Response creation failed: {e}")
            return UnifiedResponse(
                content=content,
                status=ResponseStatus.ERROR,
                response_type=response_type,
                processing_time=time.time() - start_time,
                success=False,
                error=str(e),
                error_code="RESPONSE_CREATION_ERROR",
                request_id=request_id,
            )

    def _get_response_performance_target(
        self, response_type: ResponseType
    ) -> PerformanceTarget:
        """Get appropriate performance target based on response type"""
        target_mapping = {
            ResponseType.MCP_INTEGRATION: PerformanceTarget.FAST,
            ResponseType.PERSONA_ENHANCED: PerformanceTarget.FAST,
            ResponseType.ML_PREDICTION: PerformanceTarget.ULTRA_FAST,
            ResponseType.CONVERSATIONAL: PerformanceTarget.NORMAL,
            ResponseType.DATA_QUERY: PerformanceTarget.NORMAL,
            ResponseType.SYSTEMATIC_ANALYSIS: PerformanceTarget.NORMAL,
            ResponseType.LIGHTWEIGHT_FALLBACK: PerformanceTarget.ULTRA_FAST,
            ResponseType.TRANSPARENCY: PerformanceTarget.FAST,
            ResponseType.CHAT_INTERFACE: PerformanceTarget.NORMAL,
            ResponseType.GENERIC: PerformanceTarget.NORMAL,
        }
        return target_mapping.get(response_type, PerformanceTarget.NORMAL)

    async def parse_conversational_query(
        self, query_text: str, context: Dict[str, Any] = None
    ) -> ConversationalQuery:
        """
        Parse natural language query into structured format
        Consolidates all previous query parsing logic
        """
        start_time = time.time()

        # Match query against patterns
        query_type = QueryType.GENERAL_ANALYTICS
        entities = []
        time_range = None
        filters = {}

        for pattern, config in self.query_patterns.items():
            if re.search(pattern, query_text, re.IGNORECASE):
                query_type = config["type"]

                # Extract entities
                if "entities_pattern" in config:
                    entity_matches = re.findall(
                        config["entities_pattern"], query_text, re.IGNORECASE
                    )
                    entities.extend(
                        [
                            match[1] if isinstance(match, tuple) else match
                            for match in entity_matches
                        ]
                    )

                # Extract time range
                if "time_pattern" in config:
                    time_match = re.search(
                        config["time_pattern"], query_text, re.IGNORECASE
                    )
                    if time_match:
                        time_range = f"{time_match.group(1)} {time_match.group(2)}"
                break

        # Apply context
        if context:
            if "previous_entities" in context:
                entities.extend(context["previous_entities"])
            if "default_time_range" in context and not time_range:
                time_range = context["default_time_range"]

        parse_time = (time.time() - start_time) * 1000
        self.logger.info(f"Query parsed in {parse_time:.2f}ms: {query_type.value}")

        return ConversationalQuery(
            query_type=query_type,
            entities=list(set(entities))[: self.config.max_entities_per_query],
            time_range=time_range,
            filters=filters,
            context=context or {},
        )

    async def fetch_real_time_data(self, query: ConversationalQuery) -> Dict[str, Any]:
        """
        Fetch real-time data based on parsed query
        Consolidates all data source integration patterns
        """
        cache_key = f"data_{query.query_type.value}_{hash(str(query.entities))}"

        # Check cache first
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data

        # Simulate data fetching (in real implementation, integrate with actual data sources)
        start_time = time.time()

        try:
            # Mock data based on query type
            mock_data = {
                "query_type": query.query_type.value,
                "entities": query.entities,
                "time_range": query.time_range,
                "timestamp": datetime.now().isoformat(),
                "data": self._generate_mock_data(query.query_type),
                "processing_time_ms": (time.time() - start_time) * 1000,
            }

            # Cache the result
            self.cache[cache_key] = mock_data

            return mock_data

        except Exception as e:
            self.logger.error(f"Data fetch failed: {e}")
            return {
                "error": str(e),
                "query_type": query.query_type.value,
                "timestamp": datetime.now().isoformat(),
            }

    def _generate_mock_data(self, query_type: QueryType) -> Dict[str, Any]:
        """Generate mock data based on query type"""
        if query_type == QueryType.SPRINT_METRICS:
            return {
                "velocity": 42,
                "completed_stories": 15,
                "burndown_trend": "positive",
                "team_satisfaction": 8.5,
            }
        elif query_type == QueryType.TEAM_PERFORMANCE:
            return {
                "throughput": 23,
                "cycle_time_days": 3.2,
                "quality_score": 92,
                "collaboration_index": 7.8,
            }
        elif query_type == QueryType.ROI_ANALYSIS:
            return {
                "investment": 150000,
                "returns": 280000,
                "roi_percentage": 86.7,
                "payback_period_months": 8,
            }
        else:
            return {
                "status": "healthy",
                "metrics_available": True,
                "last_updated": datetime.now().isoformat(),
            }

    async def process_conversational_query(
        self, query_text: str, context: Dict[str, Any] = None
    ) -> Tuple[ConversationalQuery, Dict[str, Any]]:
        """
        Complete pipeline: Parse query and fetch data with performance optimization
        """
        # Parse query with FAST performance target
        parsed_query = await self.optimize_request(
            lambda ctx: asyncio.run(
                self.parse_conversational_query(query_text, context)
            ),
            target=PerformanceTarget.FAST,
            context=context,
            request_id=f"parse_{hash(query_text)}",
        )

        # Fetch data with NORMAL performance target
        data_response = await self.optimize_request(
            lambda ctx: asyncio.run(self.fetch_real_time_data(parsed_query[0])),
            target=PerformanceTarget.NORMAL,
            context={"query": parsed_query[0]},
            request_id=f"fetch_{hash(str(parsed_query[0].entities))}",
        )

        return parsed_query[0], data_response[0]

    def process(self, data: Any) -> Any:
        """
        BaseManager process method - unified entry point for any processing
        """
        if isinstance(data, dict):
            if "query_text" in data:
                # Conversational query processing
                result = asyncio.run(
                    self.process_conversational_query(
                        data["query_text"], data.get("context")
                    )
                )
                return {"parsed_query": result[0], "data_response": result[1]}
            elif "request_func" in data:
                # Performance optimization
                result = asyncio.run(
                    self.optimize_request(
                        data["request_func"],
                        data.get("target", PerformanceTarget.NORMAL),
                        data.get("context"),
                        data.get("enable_cache", True),
                        data.get("request_id"),
                    )
                )
                return {"result": result[0], "metrics": result[1]}

        # Default pass-through with performance tracking
        start_time = time.time()
        processing_time = time.time() - start_time
        self.metrics.update(
            {"processing_time_ms": processing_time * 1000, "operation": "passthrough"}
        )
        return {"result": data, "metrics": self.metrics}

    def cleanup(self):
        """Clean up resources"""
        executors = [
            self.ultra_fast_executor,
            self.fast_executor,
            self.normal_executor,
            self.background_executor,
        ]
        for executor in executors:
            executor.shutdown(wait=True)


# PHASE 8.4: Unified factory functions (replace multiple manager factories)
def create_unified_data_performance_manager(
    config: Optional[UnifiedDataPerformanceConfig] = None,
) -> UnifiedDataPerformanceManager:
    """Create unified manager replacing both performance and conversational managers"""
    return UnifiedDataPerformanceManager(config)


# Legacy compatibility functions (eliminate need for separate files)
def create_response_optimizer(max_workers=4, target_response_ms=400, *args, **kwargs):
    """Legacy compatibility for response optimizer creation"""
    config = UnifiedDataPerformanceConfig(
        manager_name="response_optimizer",
        manager_type=ManagerType.PERFORMANCE,
        normal_target_ms=target_response_ms,
        normal_workers=max_workers,
    )
    return UnifiedDataPerformanceManager(config)


def create_conversational_data_manager():
    """Legacy compatibility for conversational data manager"""
    return create_unified_data_performance_manager()


def create_chat_context_manager():
    """Legacy compatibility for chat context manager"""
    return create_unified_data_performance_manager()


# PHASE 8.4: Response creation functions (replaces unified_response_handler.py - 590 lines â†’ ELIMINATED)
_unified_manager = None


def get_unified_manager() -> UnifiedDataPerformanceManager:
    """Get global unified manager instance"""
    global _unified_manager
    if _unified_manager is None:
        _unified_manager = create_unified_data_performance_manager()
    return _unified_manager


async def create_mcp_response(
    content: str, mcp_server: Optional[str] = None, **kwargs
) -> UnifiedResponse:
    """Create MCP response (replaces all MCPResponse instantiations)"""
    manager = get_unified_manager()
    return await manager.create_response(
        content=content,
        response_type=ResponseType.MCP_INTEGRATION,
        mcp_server_used=mcp_server,
        **kwargs,
    )


async def create_persona_response(
    content: str, persona: str, **kwargs
) -> UnifiedResponse:
    """Create persona response (replaces EnhancedResponse, PersonaResponse, etc.)"""
    manager = get_unified_manager()
    return await manager.create_response(
        content=content,
        response_type=ResponseType.PERSONA_ENHANCED,
        persona=persona,
        **kwargs,
    )


async def create_fallback_response(
    content: str, reason: str, **kwargs
) -> UnifiedResponse:
    """Create fallback response (replaces FallbackResponse)"""
    manager = get_unified_manager()
    return await manager.create_response(
        content=content,
        response_type=ResponseType.LIGHTWEIGHT_FALLBACK,
        status=ResponseStatus.FALLBACK,
        fallback_reason=reason,
        **kwargs,
    )


async def create_conversational_response(content: str, **kwargs) -> UnifiedResponse:
    """Create conversational response (replaces InteractionResponse, ChatResponse)"""
    manager = get_unified_manager()
    return await manager.create_response(
        content=content, response_type=ResponseType.CONVERSATIONAL, **kwargs
    )


# Aliases for backward compatibility
ResponseOptimizer = create_response_optimizer
ConversationalDataManager = UnifiedDataPerformanceManager
ChatContextManager = UnifiedDataPerformanceManager
