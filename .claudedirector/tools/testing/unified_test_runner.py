#!/usr/bin/env python3
"""
Unified Test Runner - Single Source of Truth for Test Execution
==============================================================

This runner eliminates CI/local discrepancies by providing identical test
execution across all environments (local, CI, pre-push).

Author: Martin | Platform Architecture
Purpose: 100% parity between local and GitHub CI test execution
"""

import os
import sys
import yaml
import subprocess
import time
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum


class TestStatus(Enum):
    """Test execution status"""

    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    TIMEOUT = "timeout"


class Environment(Enum):
    """Test execution environment"""

    LOCAL = "local"
    GITHUB_CI = "github_ci"
    PRE_PUSH = "pre_push"


@dataclass
class TestResult:
    """Individual test result"""

    name: str
    status: TestStatus
    duration: float
    output: str = ""
    error: str = ""
    timeout: int = 60
    critical: bool = True


@dataclass
class SuiteResult:
    """Test suite result"""

    name: str
    description: str
    status: TestStatus
    duration: float
    tests: List[TestResult]
    blocking: bool = True

    @property
    def passed_count(self) -> int:
        return len([t for t in self.tests if t.status == TestStatus.PASSED])

    @property
    def failed_count(self) -> int:
        return len([t for t in self.tests if t.status == TestStatus.FAILED])

    @property
    def total_count(self) -> int:
        return len(self.tests)


class Colors:
    """ANSI color codes for terminal output"""

    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"


class UnifiedTestRunner:
    """
    Unified test runner that provides identical behavior across all environments.

    Key features:
    - Single source of truth for test definitions (test_registry.yaml)
    - Identical execution in local, CI, and pre-push contexts
    - Environment-specific configuration with validation
    - Architecture consistency validation
    - Comprehensive reporting and logging
    """

    def __init__(
        self,
        registry_path: Optional[str] = None,
        environment: Optional[Environment] = None,
    ):
        self.project_root = Path(__file__).parent.parent.parent.parent
        self.registry_path = (
            registry_path
            or self.project_root / ".claudedirector/config/test_registry.yaml"
        )
        self.environment = environment or self._detect_environment()

        # Load test registry
        self.registry = self._load_test_registry()

        # Setup environment
        self._setup_environment()

        # Detect best Python executable (venv if available)
        self.python_executable = self._detect_python_executable()

        # Initialize tracking
        self.start_time = time.time()
        self.results: List[SuiteResult] = []

    def _detect_environment(self) -> Environment:
        """Detect current execution environment"""
        if os.getenv("GITHUB_ACTIONS"):
            return Environment.GITHUB_CI
        elif os.getenv("PRE_PUSH_HOOK"):
            return Environment.PRE_PUSH
        else:
            return Environment.LOCAL

    def _detect_python_executable(self) -> str:
        """Detect the best Python executable to use (prefer venv if available)"""
        # Check for venv Python first (for dependencies like psutil)
        venv_python = self.project_root / "venv" / "bin" / "python"
        if venv_python.exists():
            self._log(f"ðŸ Using venv Python: {venv_python}", "INFO")
            return str(venv_python)

        # Fall back to current Python executable
        self._log(f"ðŸ Using system Python: {sys.executable}", "INFO")
        return sys.executable

    def _load_test_registry(self) -> Dict[str, Any]:
        """Load test registry configuration"""
        try:
            with open(self.registry_path, "r") as f:
                registry = yaml.safe_load(f)

            self._log(f"ðŸ“‹ Loaded test registry: {self.registry_path}", "INFO")
            self._log(f"ðŸŒ Environment: {self.environment.value}", "INFO")

            return registry
        except Exception as e:
            self._log(f"âŒ Failed to load test registry: {e}", "ERROR")
            raise

    def _setup_environment(self):
        """Setup environment for consistent test execution"""
        env_config = self.registry["environments"][self.environment.value]

        # Setup Python path
        for path_addition in env_config.get("python_path_additions", []):
            abs_path = str(self.project_root / path_addition)
            if abs_path not in sys.path:
                sys.path.insert(0, abs_path)

        # Setup environment variables
        for key, value in env_config.get("environment_variables", {}).items():
            os.environ[key] = value

        self._log(f"ðŸ”§ Environment configured for {self.environment.value}", "SUCCESS")

    def _log(self, message: str, level: str = "INFO"):
        """Colored logging output"""
        colors = {
            "INFO": Colors.OKBLUE,
            "SUCCESS": Colors.OKGREEN,
            "WARNING": Colors.WARNING,
            "ERROR": Colors.FAIL,
            "HEADER": Colors.HEADER,
        }

        color = colors.get(level, Colors.OKBLUE)
        timestamp = time.strftime("%H:%M:%S")
        print(f"{color}[{timestamp}] {message}{Colors.ENDC}")

    def run_profile(self, profile_name: str) -> bool:
        """Run a complete execution profile"""
        self._log("=" * 80, "HEADER")
        self._log(f"ðŸš€ UNIFIED TEST RUNNER - {profile_name.upper()}", "HEADER")
        self._log("=" * 80, "HEADER")

        profile = self.registry["execution_profiles"].get(profile_name)
        if not profile:
            self._log(f"âŒ Unknown execution profile: {profile_name}", "ERROR")
            return False

        self._log(f"ðŸ“ Profile: {profile['description']}", "INFO")

        # Run each suite in the profile
        overall_success = True
        for suite_name in profile["suites"]:
            suite_result = self.run_suite(suite_name)
            self.results.append(suite_result)

            if suite_result.blocking and suite_result.status == TestStatus.FAILED:
                overall_success = False
                if profile.get("fail_fast", True):
                    self._log(
                        f"ðŸš« FAIL FAST: Stopping due to blocking failure in {suite_name}",
                        "ERROR",
                    )
                    break

        # Generate final report
        self._generate_final_report(overall_success)
        return overall_success

    def run_suite(self, suite_name: str) -> SuiteResult:
        """Run a complete test suite"""
        suite_config = self.registry["test_suites"].get(suite_name)
        if not suite_config:
            self._log(f"âŒ Unknown test suite: {suite_name}", "ERROR")
            return SuiteResult(suite_name, "Unknown suite", TestStatus.FAILED, 0.0, [])

        # Check if suite is enabled
        if not suite_config.get("enabled", True):
            self._log(
                f"âš ï¸ SUITE DISABLED: {suite_name} - {suite_config.get('description', '')}",
                "WARNING",
            )
            return SuiteResult(
                suite_name, suite_config["description"], TestStatus.SKIPPED, 0.0, []
            )

        self._log(f"\nðŸ§ª RUNNING TEST SUITE: {suite_name.upper()}", "HEADER")
        self._log(f"ðŸ“ {suite_config['description']}", "INFO")
        self._log(
            f"ðŸš¨ Blocking: {'YES' if suite_config.get('blocking', True) else 'NO'}",
            "INFO",
        )

        suite_start = time.time()
        test_results = []

        # Run each test in the suite
        for test_config in suite_config["tests"]:
            test_result = self._run_single_test(test_config)
            test_results.append(test_result)

            # Fail fast if configured and test is critical
            if (
                suite_config.get("fail_fast", False)
                and test_result.critical
                and test_result.status == TestStatus.FAILED
            ):
                self._log(
                    f"ðŸš« FAIL FAST: Stopping suite due to critical test failure",
                    "ERROR",
                )
                break

        suite_duration = time.time() - suite_start

        # Determine suite status
        failed_critical = [
            t for t in test_results if t.status == TestStatus.FAILED and t.critical
        ]
        if failed_critical:
            suite_status = TestStatus.FAILED
        elif any(t.status == TestStatus.FAILED for t in test_results):
            suite_status = (
                TestStatus.FAILED
                if suite_config.get("blocking", True)
                else TestStatus.PASSED
            )
        else:
            suite_status = TestStatus.PASSED

        suite_result = SuiteResult(
            name=suite_name,
            description=suite_config["description"],
            status=suite_status,
            duration=suite_duration,
            tests=test_results,
            blocking=suite_config.get("blocking", True),
        )

        # Log suite summary
        self._log_suite_summary(suite_result)
        return suite_result

    def _run_single_test(self, test_config: Dict[str, Any]) -> TestResult:
        """Run a single test"""
        test_name = test_config["name"]
        timeout = test_config.get("timeout", 60)
        critical = test_config.get("critical", True)

        self._log(f"\nðŸ§ª RUNNING: {test_name}", "INFO")
        self._log(
            f"   â±ï¸ Timeout: {timeout}s | ðŸš¨ Critical: {'YES' if critical else 'NO'}",
            "INFO",
        )

        test_start = time.time()

        try:
            if test_config.get("type") == "command":
                # Run shell command (replace python with detected executable)
                command = test_config["command"]
                if command.startswith("python "):
                    command = command.replace(
                        "python ", f"{self.python_executable} ", 1
                    )
                elif command.startswith("python3 "):
                    command = command.replace(
                        "python3 ", f"{self.python_executable} ", 1
                    )

                result = subprocess.run(
                    command,
                    shell=True,
                    capture_output=True,
                    text=True,
                    timeout=timeout,
                    cwd=self.project_root,
                )

                success = result.returncode == 0
                output = result.stdout
                error = result.stderr

            else:
                # Run Python test file
                test_path = self.project_root / test_config["path"]
                if not test_path.exists():
                    raise FileNotFoundError(f"Test file not found: {test_path}")

                result = subprocess.run(
                    [self.python_executable, str(test_path)],
                    capture_output=True,
                    text=True,
                    timeout=timeout,
                    cwd=self.project_root,
                )

                success = result.returncode == 0
                output = result.stdout
                error = result.stderr

            duration = time.time() - test_start
            status = TestStatus.PASSED if success else TestStatus.FAILED

            # Log result
            if success:
                self._log(f"   âœ… PASSED ({duration:.2f}s)", "SUCCESS")
            else:
                self._log(f"   âŒ FAILED ({duration:.2f}s)", "ERROR")
                if error:
                    self._log(f"   ðŸ“ Error: {error[:200]}...", "ERROR")

            return TestResult(
                name=test_name,
                status=status,
                duration=duration,
                output=output,
                error=error,
                timeout=timeout,
                critical=critical,
            )

        except subprocess.TimeoutExpired:
            duration = time.time() - test_start
            self._log(f"   â° TIMEOUT ({duration:.2f}s)", "ERROR")

            return TestResult(
                name=test_name,
                status=TestStatus.TIMEOUT,
                duration=duration,
                error=f"Test timed out after {timeout}s",
                timeout=timeout,
                critical=critical,
            )

        except Exception as e:
            duration = time.time() - test_start
            self._log(f"   ðŸ’¥ ERROR: {e}", "ERROR")

            return TestResult(
                name=test_name,
                status=TestStatus.FAILED,
                duration=duration,
                error=str(e),
                timeout=timeout,
                critical=critical,
            )

    def _log_suite_summary(self, suite_result: SuiteResult):
        """Log test suite summary"""
        self._log(f"\nðŸ“Š SUITE SUMMARY: {suite_result.name.upper()}", "HEADER")
        self._log(
            f"   Status: {'âœ… PASSED' if suite_result.status == TestStatus.PASSED else 'âŒ FAILED'}"
        )
        self._log(f"   Duration: {suite_result.duration:.2f}s")
        self._log(
            f"   Tests: {suite_result.passed_count}/{suite_result.total_count} passed"
        )

        # Log failed tests
        failed_tests = [t for t in suite_result.tests if t.status == TestStatus.FAILED]
        if failed_tests:
            self._log(f"\nâŒ FAILED TESTS:")
            for test in failed_tests:
                critical_marker = "ðŸš¨" if test.critical else "âš ï¸"
                self._log(f"   {critical_marker} {test.name} ({test.duration:.2f}s)")

    def _generate_final_report(self, overall_success: bool):
        """Generate comprehensive final report"""
        total_duration = time.time() - self.start_time

        self._log("\n" + "=" * 80, "HEADER")
        self._log("ðŸ“Š UNIFIED TEST RUNNER - FINAL REPORT", "HEADER")
        self._log("=" * 80, "HEADER")

        # Overall status
        if overall_success:
            self._log("ðŸŽ‰ OVERALL STATUS: SUCCESS", "SUCCESS")
        else:
            self._log("âŒ OVERALL STATUS: FAILURE", "ERROR")

        self._log(f"â±ï¸ Total Duration: {total_duration:.2f}s", "INFO")
        self._log(f"ðŸŒ Environment: {self.environment.value}", "INFO")

        # Suite summaries
        self._log("\nðŸ“‹ SUITE RESULTS:", "INFO")
        for suite in self.results:
            status_icon = (
                "âœ…"
                if suite.status == TestStatus.PASSED
                else "âŒ" if suite.status == TestStatus.FAILED else "âš ï¸"
            )
            blocking_marker = "ðŸš¨" if suite.blocking else "ðŸ“"
            self._log(
                f"   {status_icon} {blocking_marker} {suite.name}: {suite.passed_count}/{suite.total_count} tests passed ({suite.duration:.2f}s)"
            )

        # Critical failures
        critical_failures = []
        for suite in self.results:
            for test in suite.tests:
                if test.status == TestStatus.FAILED and test.critical:
                    critical_failures.append((suite.name, test.name))

        if critical_failures:
            self._log("\nðŸš¨ CRITICAL FAILURES:", "ERROR")
            for suite_name, test_name in critical_failures:
                self._log(f"   âŒ {suite_name}: {test_name}", "ERROR")

        # Save results for CI integration
        self._save_results_json()

        self._log("=" * 80, "HEADER")

    def _save_results_json(self):
        """Save results in JSON format for CI integration"""
        results_dir = self.project_root / ".claudedirector/test_results"
        results_dir.mkdir(exist_ok=True)

        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"test_results_{timestamp}.json"

        # Convert results to JSON-serializable format
        json_results = {
            "timestamp": timestamp,
            "environment": self.environment.value,
            "duration": time.time() - self.start_time,
            "overall_success": all(
                s.status != TestStatus.FAILED or not s.blocking for s in self.results
            ),
            "suites": [],
        }

        for suite in self.results:
            suite_data = {
                "name": suite.name,
                "description": suite.description,
                "status": suite.status.value,
                "duration": suite.duration,
                "blocking": suite.blocking,
                "tests": [
                    {
                        "name": test.name,
                        "status": test.status.value,
                        "duration": test.duration,
                        "critical": test.critical,
                        "error": (
                            test.error if test.status == TestStatus.FAILED else None
                        ),
                    }
                    for test in suite.tests
                ],
            }
            json_results["suites"].append(suite_data)

        with open(results_file, "w") as f:
            json.dump(json_results, f, indent=2)

        self._log(f"ðŸ’¾ Results saved: {results_file}", "INFO")

    def validate_architecture_consistency(self) -> bool:
        """Validate that test architecture remains consistent"""
        self._log("\nðŸ” VALIDATING ARCHITECTURE CONSISTENCY", "HEADER")

        consistency_checks = self.registry.get("consistency_checks", [])
        all_passed = True

        for check in consistency_checks:
            check_name = check["name"]
            self._log(f"ðŸ§ª {check_name}...", "INFO")

            # Implement specific validation logic
            if check["validation"] == "check_test_files_exist":
                passed = self._validate_test_files_exist()
            elif check["validation"] == "check_ci_yaml_matches_registry":
                passed = self._validate_ci_yaml_matches_registry()
            elif check["validation"] == "check_environment_parity":
                passed = self._validate_environment_parity()
            else:
                self._log(f"   âš ï¸ Unknown validation: {check['validation']}", "WARNING")
                continue

            if passed:
                self._log(f"   âœ… {check_name} - PASSED", "SUCCESS")
            else:
                self._log(f"   âŒ {check_name} - FAILED", "ERROR")
                all_passed = False

        return all_passed

    def _validate_test_files_exist(self) -> bool:
        """Validate that all registered test files exist"""
        missing_files = []

        for suite_name, suite_config in self.registry["test_suites"].items():
            for test_config in suite_config.get("tests", []):
                if "path" in test_config:
                    test_path = self.project_root / test_config["path"]
                    if not test_path.exists():
                        missing_files.append(test_config["path"])

        if missing_files:
            self._log(f"   Missing test files: {missing_files}", "ERROR")
            return False

        return True

    def _validate_ci_yaml_matches_registry(self) -> bool:
        """Validate that CI YAML matches test registry"""
        # For now, we trust that the unified runner ensures consistency
        # Future enhancement: Parse CI YAML and compare with registry
        return True

    def _validate_environment_parity(self) -> bool:
        """Validate that local and CI environments are identical"""
        # Environment parity is ensured through unified configuration
        # Both local and CI use the same test registry and runner
        return True


def main():
    """Main entry point for unified test runner"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Unified Test Runner - Consistent test execution across all environments"
    )
    parser.add_argument(
        "profile",
        help="Execution profile to run",
        choices=["ci_full", "pre_push", "local_quick", "regression_only"],
    )
    parser.add_argument("--registry", help="Path to test registry YAML file")
    parser.add_argument(
        "--validate",
        action="store_true",
        help="Run architecture consistency validation",
    )
    parser.add_argument(
        "--environment",
        choices=["local", "github_ci", "pre_push"],
        help="Override environment detection",
    )

    args = parser.parse_args()

    # Create runner
    environment = Environment(args.environment) if args.environment else None
    runner = UnifiedTestRunner(registry_path=args.registry, environment=environment)

    # Run validation if requested
    if args.validate:
        if not runner.validate_architecture_consistency():
            sys.exit(1)

    # Run test profile
    success = runner.run_profile(args.profile)

    # Exit with appropriate code
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
